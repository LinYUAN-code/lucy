function e(e,t){return t.map((e=>e.replaceAll(/\s/g,""))).map((t=>{const r=t.split(/(=>)|(->)/).filter((e=>"=>"!==e&&"->"!==e&&e)),n=r[1].split("|").filter((e=>e&&"|"!==e)).map((t=>e.splitDerivation(t)));return{nonTerminal:r[0],derivations:n}}))}function t(e){switch(e){case"+":case"*":case"(":case")":return`\\${e}`;default:return e}}const r="ε",n="$";var i;!function(e){e[e.Normal=0]="Normal",e[e.Warnning=1]="Warnning",e[e.Error=2]="Error",e[e.None=3]="None"}(i||(i={}));let o=i.Normal;o=i.Error;var s=new class{logLevel;logChannel;constructor(e=i.Normal,t=console){this.logLevel=e,this.logChannel=t}log(...e){this.logLevel<=i.Normal&&this.logChannel.log("[normal]",...e)}warn(...e){this.logLevel<=i.Warnning&&this.logChannel.warn("[warn]",...e)}error(...e){this.logLevel<=i.Error&&this.logChannel.error("[error]",...e)}logTo(e){this.logChannel=e}}(o);function a(t,n){s.log("[generateFirstSet start]");const i=e(t,n);s.log("[grammers after transferString2Grammers]",i);const o=new Array(...i.map((e=>({tocken:e.nonTerminal,terminals:new Set})))),a=new Map;i.forEach((e=>{a.set(e.nonTerminal,e.derivations)})),o.push(...t.terminals.map((e=>({tocken:e[0],terminals:new Set([e[0]]),isTerminal:!0})))),o.forEach((e=>{if(!e.isTerminal)for(let t of a.get(e.tocken))1!==t.length||t[0]!==r||e.terminals.has(r)||e.terminals.add(r)}));const l=new Map;for(let e of o)l.set(e.tocken,e);for(;;){let e=!1;if(o.forEach((n=>{if(!n.isTerminal)for(let i of a.get(n.tocken))for(let o=0;o<i.length;o++){const a=i[o];if(t.isTerminal(a)){if(n.terminals.has(a)||(s.log(a),e=!0,n.terminals.add(a)),a!==r)break}else{const t=l.get(a);if(t.terminals.forEach((t=>{t!==r&&(n.terminals.has(t)||(e=!0,n.terminals.add(t)))})),!t.terminals.has(r))break}o===i.length-1&&n.terminals.add(r)}})),!e)break}return o.filter((e=>!e.isTerminal)).sort(((e,t)=>e.tocken<t.tocken?-1:1)).map((e=>(e.terminals=new Set(Array.from(e.terminals).sort(((e,t)=>e<t?-1:1))),e)))}function l(e,t,n){const i=new Set;for(let o=0;o<t.length;o++){const s=t[o];if(e.isTerminal(s)){if(s!==r){i.add(s);break}}else if(n.get(s)?.terminals.forEach((e=>{i.add(e)})),!n.get(s).terminals.has(r))break;o===t.length-1&&i.add(r)}return{tocken:t.join(""),terminals:i}}class m{nonTerminals;terminals;currentLine=0;currentColumn=0;source="";constructor(e,t){this.nonTerminals=[...t],this.terminals=[...e]}setSource(e){this.source=e,this.currentLine=0,this.currentColumn=0}remainString(){return this.source.slice(this.currentColumn)}next(){if(this.currentColumn>=this.source.length)return{tocken:n,origin:n};for(let e of this.terminals){const t=this.source.slice(this.currentColumn).match(e[1]);if(t)return{tocken:e[0],origin:t[0]}}throw new Error(`[lexer next]: match next Terminal error \n sourecInput: ${this.source}\n remainString: ${this.source.slice(this.currentColumn)}`)}pop(){try{const e=this.next();return e.tocken!==n&&(this.currentColumn+=e.origin.length),e}catch(e){throw e}}nextNotEmptyTerminal(){for(;;){const e=this.next();if("whiteSpace"!==e.tocken)return e;this.currentColumn+=e.origin.length}}nextNotEmpty(e){const t=this.currentColumn;for(let t=0;t<e-1;t++)this.popNotEmptyTerminal();const r=this.nextNotEmptyTerminal();return this.currentColumn=t,r}popNotEmptyTerminal(){const e=this.nextNotEmptyTerminal();return e.tocken!==n&&(this.currentColumn+=e.origin.length),e}isTerminal(e){let t=!0;return this.nonTerminals.some((r=>r===e&&(t=!1,!0))),t}splitDerivation(e){const r=[];let n=0;const i=e;for(;e.length;){for(let t of this.nonTerminals){const n=e.match(new RegExp("^"+t));if(n){r.push(t),e=e.slice(n[0].length);break}}for(let n of this.terminals){const i=e.match(new RegExp("^"+t(n[0])));if(i){r.push(n[0]),e=e.slice(i[0].length);break}}if(n++,n>5e4)throw new Error(`[splitDerivation] error: excute over MAX_EXCUTE str: ${i}  remaining str: ${e} `)}return r}getNewNonTerminal(e){let t=e;for(;;)if(t+="'",-1===this.nonTerminals.indexOf(t))return this.nonTerminals.unshift(t),t}}function c(t,n,i,o){const a=e(t,n),m=[],c=new Map;t.nonTerminals.forEach((e=>{const t={nonTerminal:e,terminal2Derivation:new Map};c.set(e,t),m.push(t)}));const f=new Map,h=new Map;for(let e of o)h.set(e.tocken,e);for(let e of i)f.set(e.tocken,e);return a.forEach((e=>{for(let n of e.derivations){const i=l(t,n,f),o=c.get(e.nonTerminal);s.log(i);for(let t of i.terminals){if(t===r)continue;let i=o?.terminal2Derivation.get(t);i||(i={nonTerminal:e.nonTerminal,derivations:[]},o?.terminal2Derivation.set(t,i)),i.derivations.push(n)}if(i.terminals.has(r))for(let t of h.get(e.nonTerminal).terminals){let r=o?.terminal2Derivation.get(t);r||(r={nonTerminal:e.nonTerminal,derivations:[]},o?.terminal2Derivation.set(t,r)),r.derivations.push(n)}}})),m}function f(e,t){for(let r of e.terminals){const e=r[0];t.forEach((t=>{const r=t.terminal2Derivation.get(e);if(r&&r.derivations.length>1)return!1}))}return!0}class h{lexer;textGrammers;constructor(e,t,r){this.lexer=new m(e,t),this.textGrammers=r}getFirstSet(){return a(this.lexer,this.textGrammers)}getFollowSet(t){return t||(t=this.getFirstSet()),function(t,i,o){s.log("[generateFllowSet start]"),o=o?Array.from(o):a(t,i);const l=e(t,i);s.log("[grammers after transferString2Grammers]",l);const m=new Array(...l.map((e=>({tocken:e.nonTerminal,terminals:new Set([n])})))),c=new Map;l.forEach((e=>{c.set(e.nonTerminal,e.derivations)})),m.push(...t.terminals.map((e=>({tocken:e[0],terminals:new Set([n]),isTerminal:!0}))));const f=new Map;for(let e of m)f.set(e.tocken,e);o.push(...t.terminals.map((e=>({tocken:e[0],terminals:new Set([e[0]]),isTerminal:!0}))));const h=new Map;for(let e of o)h.set(e.tocken,e);for(;;){let e=!1;if(l.forEach((t=>{for(let n of t.derivations)for(let t=n.length-2;t>=0;t--){const i=f.get(n[t]);for(let o=t+1;o<n.length;o++){const t=h.get(n[o]).terminals;for(let n of t)n!==r&&(i?.terminals.has(n)||(e=!0,i?.terminals.add(n)));if(!t.has(r))break}}})),l.forEach((t=>{const n=f.get(t.nonTerminal);for(let i of t.derivations)for(let t=i.length-1;t>=0;t--){const o=i[t],s=f.get(o);for(let t of n.terminals)t!==r&&(s?.terminals.has(t)||(e=!0,s?.terminals.add(t)));if(!h.get(o).terminals.has(r))break}})),!e)break}return m.filter((e=>!e.isTerminal)).sort(((e,t)=>e.tocken<t.tocken?-1:1)).map((e=>(e.terminals=new Set(Array.from(e.terminals).sort(((e,t)=>e<t?-1:1))),e)))}(this.lexer,this.textGrammers,t)}getPredictTable(e,t){return e||(e=this.getFirstSet()),t||(t=this.getFollowSet(e)),c(this.lexer,this.textGrammers,e,t)}getPredictProcess(e,t,i){return i||(i=c(this.lexer,this.textGrammers,this.getFirstSet(),this.getFollowSet())),function(e,t,i,o){let a=i.replaceAll(/\s/g,"");const l=[];let m={parseStack:[n,o],remainingInput:i,parseAction:""};e.setSource(a);const c=new Map;for(t.forEach((e=>{c.set(e.nonTerminal,e)}));;){const t=m.parseStack[m.parseStack.length-1];if(e.isTerminal(t)){const r=e.next();if(t!==r.tocken)throw new Error(`[predict error] terminal match error tocken: ${t} stack: ${m.parseStack} remainingInput: ${e.remainString()}`);if(m.parseAction=`match ${r.tocken} ${r.origin}`,l.push(m),m=JSON.parse(JSON.stringify(m)),e.pop(),m.parseAction="",m.remainingInput=e.remainString(),m.parseStack.pop(),0===m.parseStack.length)break;continue}const n=e.next(),i=c.get(t).terminal2Derivation.get(n.tocken);if(1!==i.derivations.length)throw new Error(`[predict error] parse input fail \n terminal: ${n} \n  remainingInput: ${e.remainString()} \n grammer: ${i} `);m.parseAction=`Predict ${i.nonTerminal} => ${i.derivations[0].join(" ")}`,s.log("[predict State]",m),l.push(m),m=JSON.parse(JSON.stringify(m)),m.parseStack.pop(),m.parseStack.push(...i.derivations[0].filter((e=>e!==r)).reverse()),m.parseAction="",m.remainingInput=e.remainString()}return l}(this.lexer,i,e,t)}checkPredickTableIsValid(e){return f(this.lexer,e)}checkIsLL0(){return f(this.lexer,this.getPredictTable())}getFirstSetProgressive(){return function*(t,n){yield["1. 如果X式一个终结符号，那么FIRST(X) = X ","2. 如果 X => ε 是一个产生式，那么将e加人到 FIRST（X)中。","3. A => B0B1B2B3\n            i = 0\n            FIRST(Bi) - EmptyCharacter 加入到 FIRST(A)中\n            如果FIRST(B1)不含有EmptyCharacter退出循环\n            若B0 - B3均含有EmptyCharacter 将EmptyCharacter加入到FIRST(A)中\n        ","4. 去除所有终结符号的表项"],s.log("[generateFirstSet start]");const i=e(t,n);s.log("[grammers after transferString2Grammers]",i);const o=new Array(...i.map((e=>({tocken:e.nonTerminal,terminals:new Set})))),a=new Map;i.forEach((e=>{a.set(e.nonTerminal,e.derivations)})),o.push(...t.terminals.map((e=>({tocken:e[0],terminals:new Set([e[0]]),isTerminal:!0})))),yield{ruleIndex:0,result:o},o.forEach((e=>{if(!e.isTerminal)for(let t of a.get(e.tocken))1!==t.length||t[0]!==r||e.terminals.has(r)||e.terminals.add(r)})),yield{ruleIndex:1,result:o};const l=new Map;for(let e of o)l.set(e.tocken,e);for(;;){let e=!1;if(o.forEach((n=>{if(!n.isTerminal)for(let i of a.get(n.tocken))for(let o=0;o<i.length;o++){const a=i[o];if(t.isTerminal(a)){if(n.terminals.has(a)||(s.log(a),e=!0,n.terminals.add(a)),a!==r)break}else{const t=l.get(a);if(t.terminals.forEach((t=>{t!==r&&(n.terminals.has(t)||(e=!0,n.terminals.add(t)))})),!t.terminals.has(r))break}o===i.length-1&&n.terminals.add(r)}})),!e)break;yield{ruleIndex:2,result:o}}yield{ruleIndex:3,result:o.filter((e=>!e.isTerminal)).sort(((e,t)=>e.tocken<t.tocken?-1:1)).map((e=>(e.terminals=new Set(Array.from(e.terminals).sort(((e,t)=>e<t?-1:1))),e)))}}(this.lexer,this.textGrammers)}getFollowSetProgressive(t){return t||(t=this.getFirstSet()),function*(t,i,o){yield["1. 将$放到FOLLOW(S)中","2. 如果存在一个产生式A => aBb ， 那么FIRST(b) 中除ε 之外的所有符号都在FOLLOW(B)中。attention: A => aBCd 那么把first(Cd)加入到Follow(B)中去","3.如果存在一个产生式 A => aB ， 或存在产生式 A => aBb 且FIRST(b) 包含 ε ，那么FOLLOW(A)中的所有符号都在FOLLOW(B)中。","4. 去除所有终结符号的表项"],s.log("[generateFllowSet start]"),o=o?Array.from(o):a(t,i);const l=e(t,i);s.log("[grammers after transferString2Grammers]",l);const m=new Array(...l.map((e=>({tocken:e.nonTerminal,terminals:new Set([n])})))),c=new Map;l.forEach((e=>{c.set(e.nonTerminal,e.derivations)})),m.push(...t.terminals.map((e=>({tocken:e[0],terminals:new Set([n]),isTerminal:!0})))),yield{ruleIndex:0,result:m};const f=new Map;for(let e of m)f.set(e.tocken,e);o.push(...t.terminals.map((e=>({tocken:e[0],terminals:new Set([e[0]]),isTerminal:!0}))));const h=new Map;for(let e of o)h.set(e.tocken,e);for(;;){let e=!1;if(l.forEach((t=>{for(let n of t.derivations)for(let t=n.length-2;t>=0;t--){const i=f.get(n[t]);for(let o=t+1;o<n.length;o++){const t=h.get(n[o]).terminals;for(let n of t)n!==r&&(i?.terminals.has(n)||(e=!0,i?.terminals.add(n)));if(!t.has(r))break}}})),e&&(yield{ruleIndex:1,result:m},e=!1),l.forEach((t=>{const n=f.get(t.nonTerminal);for(let i of t.derivations)for(let t=i.length-1;t>=0;t--){const o=i[t],s=f.get(o);for(let t of n.terminals)t!==r&&(s?.terminals.has(t)||(e=!0,s?.terminals.add(t)));if(!h.get(o).terminals.has(r))break}})),e&&(yield{ruleIndex:2,result:m}),!e)break}yield{ruleIndex:3,result:m.filter((e=>!e.isTerminal)).sort(((e,t)=>e.tocken<t.tocken?-1:1)).map((e=>(e.terminals=new Set(Array.from(e.terminals).sort(((e,t)=>e<t?-1:1))),e)))}}(this.lexer,this.textGrammers,t)}getPredictTableProgressive(t,n){return t||(t=this.getFirstSet()),n||(n=this.getFollowSet(t)),function*(t,n,i,o){yield["1. 对 First(u) 中的所有终结符 a （不含 ε ），置 M[A, a] = A -> u","2. 若 First(u) 含 ε ，则对 Follow(A) 中的所有符号 a （可含 $ ），置 M[A, a] = A -> u"];const a=e(t,n),m=[],c=new Map;t.nonTerminals.forEach((e=>{const t={nonTerminal:e,terminal2Derivation:new Map};c.set(e,t),m.push(t)}));const f=new Map,h=new Map;for(let e of o)h.set(e.tocken,e);for(let e of i)f.set(e.tocken,e);for(let e of a)for(let n of e.derivations){const i=l(t,n,f),o=c.get(e.nonTerminal);s.log(i);for(let t of i.terminals){if(t===r)continue;let i=o?.terminal2Derivation.get(t);i||(i={nonTerminal:e.nonTerminal,derivations:[]},o?.terminal2Derivation.set(t,i)),i.derivations.push(n),yield{ruleIndex:0,result:m}}if(i.terminals.has(r))for(let t of h.get(e.nonTerminal).terminals){let r=o?.terminal2Derivation.get(t);r||(r={nonTerminal:e.nonTerminal,derivations:[]},o?.terminal2Derivation.set(t,r)),r.derivations.push(n),yield{ruleIndex:1,result:m}}}}(this.lexer,this.textGrammers,t,n)}}const g=/^[A-Z]'*/,u=/[a-z|\u0391-\u03C9]/;function p(e){const t=new Set,r=new Set;return e.forEach((e=>{const n=e.replaceAll(/\s/g,"").split(/(=>)|(->)/).filter((e=>"=>"!==e&&"->"!==e&&e)),i=n[0];t.add(i),n[1].split("|").filter((e=>e&&"|"!==e)).forEach((e=>{for(;e.length;){let n=null;if(n=e.match(g),n)t.add(n[0]),e=e.slice(n[0].length);else{if(n=e.match(u),!n)throw new Error(`[getTockFromSimpleGrammers error] cant recognize the character remaining: ${e}`);r.add(JSON.stringify([n[0],"^"+n[0]])),e=e.slice(n[0].length)}}}))})),{nonTerminals:Array.from(t).sort(((e,t)=>t.length-e.length)),terminals:Array.from(r).map((e=>{const t=JSON.parse(e);return t[1]=new RegExp(t[1]),t}))}}function d(e){let t=new Map;const r=[];e.forEach((e=>{const r=(e=e.replaceAll(/\s/g,"")).split(/(=>)|(->)/).filter((e=>"=>"!==e&&"->"!==e&&e)),n=r[0],i=r[1];if(t.has(n)){const e=t.get(n);e?.push(i),t.set(n,e)}else t.set(n,[i])}));for(let e of t.keys()){const n=[...new Set(t.get(e))];r.push(`${e} => ${n.join("|")}`.split("|").join(" | "))}return s.log(r),r}function T(e,t,r){let n=e;if(!t||!r){const n=p(e);t=n.nonTerminals,r=n.terminals}let i=new m(r,t);for(;;){let e=[];const t=n.map((t=>{const r=(t=t.replaceAll(/\s/g,"")).split(/(=>)|(->)/).filter((e=>"=>"!==e&&"->"!==e&&e)),o=r[0],a=r[1].split("|").filter((e=>e)).map((e=>(s.log("[debug]",n),i.splitDerivation(e)))),l=[],m=new Map;a.forEach((e=>{let t=m.get(e[0]);t?t.push(e.slice(1)):t=[e.slice(1)],m.set(e[0],t)}));for(let t of m.keys()){if(1===m.get(t)?.length){l.push(t+m.get(t)[0].join(""));continue}const r=m.get(t),n=i.getNewNonTerminal(o);l.push(t+n),e.push(n+" => "+r?.map((e=>e.join(""))).join(" | "))}return o+" => "+l.join(" | ")}));s.log("[pre]",t,e);const r=[...t,...e];if(s.log("[com]",n,r),n.length===r.length)break;n=r}return n}function w(e,t,n){const i=[];if(!t||!n){const r=p(e);t=r.nonTerminals,n=r.terminals}s.log("[nonTerminals]",t),s.log("[terminals]",n);let o=new m(n,t);const a=new Map;for(let t of e){t=t.replaceAll(/\s/g,"");const e=t.split(/(=>)|(->)/).filter((e=>"=>"!==e&&"->"!==e&&e)),r=e[0],n=e[1].split("|").filter((e=>e)).map((e=>o.splitDerivation(e)));a.set(r,n)}s.log("[nonTerminals2DerivationMap]",a);for(let e=0;e<t.length;e++){const n=a.get(t[e]);for(let r=0;r<e;r++){const e=a.get(t[r]);for(let i=n.length-1;i>=0;i--){const o=n[i];if(o[0]===t[r]){s.log(o,t[r]);for(let t of e)s.log("[-]",t),n?.push([...t,...o.slice(1)]);n?.splice(i,1)}}}const i=[];for(let r=n.length-1;r>=0;r--){n[r][0]===t[e]&&i.push(r)}if(i.length){const s=o.getNewNonTerminal(t[e]),l=[],m=[];for(let e=n.length-1;e>=0;e--)-1===i.indexOf(e)&&l.push([...n[e][0]===r?n[e].slice(1):n[e],s]);for(let e of i)m.push([...n[e].slice(1),s]);m.push([r]),a.set(t[e],[...l,...i.length?[]:n]),a.set(s,m)}s.log("[nonTerminals2DerivationMap in process]",a,n)}for(let e of a.keys()){const t=a.get(e).map((e=>e.join(""))).join(" | ");i.push(`${e} => ${t}`)}return i}export{h as LL1Parser,m as Lexer,w as clearRightRecursion,p as getTockFromSimpleGrammers,T as liftUpCommonTocken,d as unionGrammers};
